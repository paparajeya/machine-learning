{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python program using NumPy that implements three different neural networks:\n",
    "\n",
    "- No Hidden Layer: Logistic regression-like model.\n",
    "- One Hidden Layer: Single-layer neural network with ReLU activation.\n",
    "- Two Hidden Layers: Two-layer neural network with ReLU activation for hidden layers.\n",
    "\n",
    "Each implementation includes forward and backward propagation. The models are trained using the gradient descent algorithm. The code is vectorized using NumPy to improve performance.\n",
    "\n",
    "This program demonstrates:\n",
    "\n",
    "- Initialization of weights and biases for different network configurations.\n",
    "- Forward propagation for computing activations layer-by-layer.\n",
    "- Backward propagation for calculating gradients and updating weights.\n",
    "- Training and prediction for evaluating the model.\n",
    "\n",
    "You can modify the number of layers and nodes in the network by adjusting the layers and activations variables. The training example uses a simple circle pattern for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary packages\n",
    "%pip install numpy matplotlib\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods Used:\n",
    "Some of the methods used in this program are:\n",
    "- `initialize_parameters`: Initializes weights and biases for each layer.\n",
    "- `sigmoid`: Computes the sigmoid activation function.\n",
    "- `relu`: Computes the ReLU activation function.\n",
    "- `forward_propagation`: Computes the forward propagation for the network.\n",
    "- `compute_cost`: Computes the cross-entropy loss.\n",
    "- `backward_propagation`: Computes the backward propagation for the network.\n",
    "- `update_parameters`: Updates the weights and biases using the gradients.\n",
    "- `train_model`: Trains the neural network using gradient descent.\n",
    "- `predict`: Predicts the output for a given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def initialize_weights(layers):\n",
    "    weights = {}\n",
    "    for i in range(len(layers) - 1):\n",
    "        weights[f\"W{i+1}\"] = np.random.randn(layers[i], layers[i+1]) * 0.01\n",
    "        weights[f\"b{i+1}\"] = np.zeros((1, layers[i+1]))\n",
    "    return weights\n",
    "\n",
    "def forward_propagation(X, weights, layers, activations):\n",
    "    caches = {\"A0\": X}\n",
    "    for i in range(1, len(layers)):\n",
    "        Z = caches[f\"A{i-1}\"].dot(weights[f\"W{i}\"]) + weights[f\"b{i}\"]\n",
    "        if activations[i-1] == \"relu\":\n",
    "            A = relu(Z)\n",
    "        elif activations[i-1] == \"sigmoid\":\n",
    "            A = sigmoid(Z)\n",
    "        caches[f\"Z{i}\"] = Z\n",
    "        caches[f\"A{i}\"] = A\n",
    "    return caches\n",
    "\n",
    "def backward_propagation(Y, weights, caches, layers, activations):\n",
    "    gradients = {}\n",
    "    m = Y.shape[0]\n",
    "    dA = -(Y / caches[f\"A{len(layers)-1}\"] - (1 - Y) / (1 - caches[f\"A{len(layers)-1}\"]))\n",
    "    for i in range(len(layers) - 1, 0, -1):\n",
    "        dZ = dA\n",
    "        if activations[i-1] == \"sigmoid\":\n",
    "            dZ *= sigmoid_derivative(caches[f\"Z{i}\"])\n",
    "        elif activations[i-1] == \"relu\":\n",
    "            dZ *= relu_derivative(caches[f\"Z{i}\"])\n",
    "        gradients[f\"dW{i}\"] = caches[f\"A{i-1}\"].T.dot(dZ) / m\n",
    "        gradients[f\"db{i}\"] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        dA = dZ.dot(weights[f\"W{i}\"].T)\n",
    "    return gradients\n",
    "\n",
    "def update_weights(weights, gradients, learning_rate):\n",
    "    for key in weights.keys():\n",
    "        weights[key] -= learning_rate * gradients[f\"d{key}\"]\n",
    "    return weights\n",
    "\n",
    "def train(X, Y, layers, activations, epochs=1000, learning_rate=0.01):\n",
    "    weights = initialize_weights(layers)\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        caches = forward_propagation(X, weights, layers, activations)\n",
    "        gradients = backward_propagation(Y, weights, caches, layers, activations)\n",
    "        weights = update_weights(weights, gradients, learning_rate)\n",
    "        if epoch % 100 == 0:\n",
    "            loss = -np.mean(Y * np.log(caches[f\"A{len(layers)-1}\"]) + (1 - Y) * np.log(1 - caches[f\"A{len(layers)-1}\"]))\n",
    "            losses.append(loss)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "    return weights, losses\n",
    "\n",
    "def predict(X, weights, layers, activations):\n",
    "    caches = forward_propagation(X, weights, layers, activations)\n",
    "    predictions = caches[f\"A{len(layers)-1}\"] > 0.5\n",
    "    return predictions\n",
    "\n",
    "def plot_decision_boundary(X, Y, weights, layers, activations):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    predictions = predict(grid, weights, layers, activations).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, predictions, alpha=0.8, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y.flatten(), edgecolor='k', cmap=plt.cm.Spectral)\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
