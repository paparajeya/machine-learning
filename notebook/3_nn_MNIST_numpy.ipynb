{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python script for classifying the MNIST dataset using NumPy and Matplotlib. It includes:\n",
    "\n",
    "- Data loading and preprocessing: Load the MNIST dataset and normalize the images.\n",
    "- Neural network implementation: A simple fully connected neural network with one hidden layer.\n",
    "- Training and evaluation: Use forward propagation, backward propagation, and weight updates for training.\n",
    "- Visualization: Plot training loss and some sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary packages\n",
    "%pip install numpy matplotlib torchvision\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "- Input layer: 784 neurons (28x28 pixels)\n",
    "- Hidden layer: 128 neurons with ReLU activation\n",
    "- Output layer: 10 neurons with softmax activation\n",
    "- Loss function: Cross-entropy loss\n",
    "- Optimization: Stochastic gradient descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    weights = {\n",
    "        \"W1\": np.random.randn(input_size, hidden_size) * 0.01,\n",
    "        \"b1\": np.zeros((1, hidden_size)),\n",
    "        \"W2\": np.random.randn(hidden_size, output_size) * 0.01,\n",
    "        \"b2\": np.zeros((1, output_size))\n",
    "    }\n",
    "    return weights\n",
    "\n",
    "def forward_propagation(X, weights):\n",
    "    Z1 = X.dot(weights[\"W1\"]) + weights[\"b1\"]\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = A1.dot(weights[\"W2\"]) + weights[\"b2\"]\n",
    "    A2 = softmax(Z2)\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n",
    "\n",
    "def compute_loss(Y, A2):\n",
    "    m = Y.shape[0]\n",
    "    log_probs = -np.log(A2[range(m), np.argmax(Y, axis=1)])\n",
    "    loss = np.sum(log_probs) / m\n",
    "    return loss\n",
    "\n",
    "def backward_propagation(X, Y, weights, cache):\n",
    "    m = X.shape[0]\n",
    "    A1, A2 = cache[\"A1\"], cache[\"A2\"]\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = A1.T.dot(dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "    dA1 = dZ2.dot(weights[\"W2\"].T)\n",
    "    dZ1 = dA1 * relu_derivative(cache[\"Z1\"])\n",
    "    dW1 = X.T.dot(dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "    gradients = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return gradients\n",
    "\n",
    "def update_weights(weights, gradients, learning_rate):\n",
    "    for key in weights.keys():\n",
    "        weights[key] -= learning_rate * gradients[f\"d{key}\"]\n",
    "    return weights\n",
    "\n",
    "def train(X, Y, input_size, hidden_size, output_size, epochs=10, learning_rate=0.1):\n",
    "    weights = initialize_weights(input_size, hidden_size, output_size)\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        A2, cache = forward_propagation(X, weights)\n",
    "        loss = compute_loss(Y, A2)\n",
    "        gradients = backward_propagation(X, Y, weights, cache)\n",
    "        weights = update_weights(weights, gradients, learning_rate)\n",
    "        losses.append(loss)\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "    return weights, losses\n",
    "\n",
    "def predict(X, weights):\n",
    "    A2, _ = forward_propagation(X, weights)\n",
    "    predictions = np.argmax(A2, axis=1)\n",
    "    return predictions\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    one_hot = np.zeros((y.size, num_classes))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset using torchvision\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from DataLoader\n",
    "train_data = next(iter(train_loader))\n",
    "test_data = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[0].numpy().reshape(-1, 28 * 28)\n",
    "Y_train = one_hot_encode(train_data[1].numpy(), 10)\n",
    "\n",
    "X_test = test_data[0].numpy().reshape(-1, 28 * 28)\n",
    "Y_test = one_hot_encode(test_data[1].numpy(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 784\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "epochs = 10\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "weights, losses = train(X_train, Y_train, input_size, hidden_size, output_size, epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "predictions = predict(X_test, weights)\n",
    "accuracy = np.mean(predictions == np.argmax(Y_test, axis=1))\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap=\"gray\")\n",
    "    plt.title(f\"Pred: {predictions[i]}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
